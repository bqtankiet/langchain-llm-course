{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 1: Model, Prompts and Parser\n",
        "This notebook documents my learning journey on **LangChain for LLM Application Development** course from Deeplearning.ai \\\n",
        "[Lesson 1: Model, Prompts and Parser](https://learn.deeplearning.ai/courses/langchain/lesson/xf7wh/models,-prompts-and-parsers).\n",
        "\n",
        "\\\n",
        "What I Learned\n",
        "- How to set up the environment: Installing necessary libraries like `python-dotenv` and `langchain-groq`, and managing API keys securely.\n",
        "- How to interact with LLMs: Using `init_chat_model` to connect to a language model and send prompts.\n",
        "- How to use Prompt Templates: Creating dynamic and reusable prompts with `ChatPromptTemplate` to guide the LLM's output.\n",
        "- How to structure LLM output: Using `StructuredOutputParser` and `ResponseSchema` to get structured JSON data from the LLM.\n",
        "- How to chain components with LCEL: Using the pipe operator (`|`) to create a clean and readable pipeline of operations."
      ],
      "metadata": {
        "id": "tn5psMoOrZwq"
      },
      "id": "tn5psMoOrZwq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Setting up the Environment\n",
        "\n",
        "First, we need to install a few libraries:"
      ],
      "metadata": {
        "id": "WgQKGUL6vb5q"
      },
      "id": "WgQKGUL6vb5q"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NPo53JEzv6jT"
      },
      "id": "NPo53JEzv6jT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain langchain-groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mR9aapBnmxfK",
        "outputId": "e91381d8-df39-4e7c-8b0d-c9e3ab1ac14a"
      },
      "id": "mR9aapBnmxfK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.27\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-groq\n",
            "Version: 0.3.6\n",
            "Summary: An integration package connecting Groq and LangChain\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: groq, langchain-core\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create `.env` file and\n",
        "replace `secret-api-key` with your actual credentials.\n",
        "```\n",
        "GROQ_API_KEY=secret-api-key\n",
        "```"
      ],
      "metadata": {
        "id": "cqlWvXUDGLIq"
      },
      "id": "cqlWvXUDGLIq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads environment variables from the .env file"
      ],
      "metadata": {
        "id": "921xrxD4w6Bc"
      },
      "id": "921xrxD4w6Bc"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "_ = load_dotenv(override=True) # read local .env file"
      ],
      "metadata": {
        "id": "PYkKaSkVw7K8"
      },
      "id": "PYkKaSkVw7K8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28e07d37"
      },
      "source": [
        "## II. Interacting with LLMs\n",
        "\n",
        "Now that we have our environment set up, let's start interacting with a language model. We'll use the function `init_chat_model()` from `langchain.chat_models` to connect to a specific LLM model. In this case, I'm using `llama-3.3-70b-versatile`"
      ],
      "id": "28e07d37"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to set some model configurations."
      ],
      "metadata": {
        "id": "eJ4ezckNx_vN"
      },
      "id": "eJ4ezckNx_vN"
    },
    {
      "cell_type": "code",
      "source": [
        "# configs\n",
        "configs = {\n",
        "    \"model\": \"llama-3.3-70b-versatile\",\n",
        "    \"model_provider\": \"groq\",\n",
        "    \"temperature\": 0\n",
        "}"
      ],
      "metadata": {
        "id": "U8xzKXSLxsVT"
      },
      "id": "U8xzKXSLxsVT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializes the chat model and sends a simple message to see a response."
      ],
      "metadata": {
        "id": "lpQFNnkkyNEy"
      },
      "id": "lpQFNnkkyNEy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc4f178d",
        "outputId": "1e7e0a35-68f5-4738-e1b3-640802c4683b"
      },
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# Initialize the model\n",
        "llm = init_chat_model(**configs)\n",
        "\n",
        "# Simple chat interaction\n",
        "messages = [HumanMessage(content=\"What is 1 + 1?\")] # remember: messages must be a list\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "id": "fc4f178d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 + 1 = 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just see what type the chat model is\n",
        "print(type(llm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cibj6u7_tSt0",
        "outputId": "5cb6888e-9fe3-425d-a9a8-baf874a9ee0d"
      },
      "id": "Cibj6u7_tSt0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_groq.chat_models.ChatGroq'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Prompt template\n",
        "Prompt Templates in LangChain allow you to create flexible, reusable prompts for interacting with language models. You define a structure with placeholders and fill them with specific information each time."
      ],
      "metadata": {
        "id": "9UcHIo5a0IIE"
      },
      "id": "9UcHIo5a0IIE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a template with placeholders"
      ],
      "metadata": {
        "id": "I8w1vOiRBi5_"
      },
      "id": "I8w1vOiRBi5_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `{}` to create placeholders in the prompt template."
      ],
      "metadata": {
        "id": "opquHACl1dHv"
      },
      "id": "opquHACl1dHv"
    },
    {
      "cell_type": "code",
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by the keyword \"text\" \\\n",
        "into a style specified by the keyword \"style\"\n",
        "\n",
        "style: {style}\n",
        "\n",
        "text: {text}\n",
        "\n",
        "provide the translated sentence directly as the response\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l2ail5jO1RWY"
      },
      "id": "l2ail5jO1RWY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `ChatPromptTemplate.from_template()` to create a prompt template from a string"
      ],
      "metadata": {
        "id": "236n3i3u14nj"
      },
      "id": "236n3i3u14nj"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ],
      "metadata": {
        "id": "bBNiMFRL1iD8"
      },
      "id": "bBNiMFRL1iD8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain, the `prompt_template.messages` is a list because a prompt can include multiple messages, allowing for a flexible, multi-step conversation structure. Each message in the list represents a different part of the interaction (e.g., user input or model response).  \\\n",
        "Since there's only one message, we use `[0]` to access it directly."
      ],
      "metadata": {
        "id": "tFqj3Ojj3MCf"
      },
      "id": "tFqj3Ojj3MCf"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzSNNoJj1_HA",
        "outputId": "1e6e046d-a48e-495b-a683-76afcc504818"
      },
      "id": "ZzSNNoJj1_HA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by the keyword \"text\" into a style specified by the keyword \"style\"\\n\\nstyle: {style}\\n\\ntext: {text}\\n\\nprovide the translated sentence directly as the response\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template.messages[0].prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrJyn2ET2HVU",
        "outputId": "9b760d03-2f04-4bb6-cc5e-193ba9708ab7"
      },
      "id": "WrJyn2ET2HVU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['style', 'text']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fill in the placeholders"
      ],
      "metadata": {
        "id": "6g2mNQl6Bqa5"
      },
      "id": "6g2mNQl6Bqa5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define values for `style` and `text` placeholders, like `customer_style` and `customer_email`.\n"
      ],
      "metadata": {
        "id": "bSq8h1HJ3e5U"
      },
      "id": "bSq8h1HJ3e5U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "_q5yAFyD4XXP"
      },
      "outputs": [],
      "source": [
        "customer_style = \"a calm and respectful tone in American English\""
      ],
      "id": "_q5yAFyD4XXP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "IcfJ3mjW4XXQ"
      },
      "outputs": [],
      "source": [
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\""
      ],
      "id": "IcfJ3mjW4XXQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `format_messages()` to fill in the placeholders of our `prompt_template` with the defined values (`customer_style` and `customer_email`). This returns a list of formatted messages, and we can then inspect the type of `customer_messages` and access the first message using `[0]`."
      ],
      "metadata": {
        "id": "mmpzM-3H4zPy"
      },
      "id": "mmpzM-3H4zPy"
    },
    {
      "cell_type": "code",
      "source": [
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)"
      ],
      "metadata": {
        "id": "3V2CQ5wc41uh"
      },
      "id": "3V2CQ5wc41uh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just see what type it is\n",
        "print(type(customer_messages))\n",
        "print(type(customer_messages[0]))"
      ],
      "metadata": {
        "id": "oUldsv4G45fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fbdfdf-a4aa-4426-9855-771b38ef3b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n"
          ]
        }
      ],
      "id": "oUldsv4G45fK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Just see the content that we will pass to the model\n",
        "print(customer_messages[0].content)"
      ],
      "metadata": {
        "id": "niYbjxX95VHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5192a892-8bc2-46a3-b259-395798f270f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by the keyword \"text\" into a style specified by the keyword \"style\"\n",
            "\n",
            "style: a calm and respectful tone in American English\n",
            "\n",
            "text: \n",
            "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
            "\n",
            "\n",
            "provide the translated sentence directly as the response\n",
            "\n"
          ]
        }
      ],
      "id": "niYbjxX95VHm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pass the prompt to the LLM"
      ],
      "metadata": {
        "id": "PBW_hE5pCAyP"
      },
      "id": "PBW_hE5pCAyP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calls the language model to generate a response matching the customer's tone and style, then prints the output."
      ],
      "metadata": {
        "id": "28Q5rc_h6RZ3"
      },
      "id": "28Q5rc_h6RZ3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "FM3gpt8h6Mfn"
      },
      "outputs": [],
      "source": [
        "customer_response = llm.invoke(customer_messages)"
      ],
      "id": "FM3gpt8h6Mfn"
    },
    {
      "cell_type": "code",
      "source": [
        "print(customer_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juDWDeqIRy2g",
        "outputId": "7f582195-87b4-4cfb-dfb2-2b6a67964441"
      },
      "id": "juDWDeqIRy2g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm extremely frustrated that my blender lid came off and splattered my kitchen walls with smoothie, and to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen, so I would greatly appreciate your assistance with this issue as soon as possible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is another example showing the reusability of the prompt template. We reuse the same template to format a different message (`service_reply`), but this time with a **Pirate English** style (`service_style_pirate`). This demonstrates how a single template can be adapted for various inputs and styles."
      ],
      "metadata": {
        "id": "EyBATVR67Bno"
      },
      "id": "EyBATVR67Bno"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "yFopuOTA7MHw"
      },
      "outputs": [],
      "source": [
        "service_reply = \"\"\"Hey there customer, \\\n",
        "the warranty does not cover \\\n",
        "cleaning expenses for your kitchen \\\n",
        "because it's your fault that \\\n",
        "you misused your blender \\\n",
        "by forgetting to put the lid on before \\\n",
        "starting the blender. \\\n",
        "Tough luck! See ya!\n",
        "\"\"\""
      ],
      "id": "yFopuOTA7MHw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "TWeAcQnc7MHx"
      },
      "outputs": [],
      "source": [
        "service_style_pirate = \"a polite tone that speaks in English Pirate\""
      ],
      "id": "TWeAcQnc7MHx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2c3604-39fe-41b5-b8bf-1823af0dab74",
        "id": "S8N3mA7z7MHx"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by the keyword \"text\" into a style specified by the keyword \"style\"\n",
            "\n",
            "style: a polite tone that speaks in English Pirate\n",
            "\n",
            "text: Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
            "\n",
            "\n",
            "provide the translated sentence directly as the response\n",
            "\n"
          ]
        }
      ],
      "source": [
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style_pirate,\n",
        "    text=service_reply)\n",
        "\n",
        "print(service_messages[0].content)"
      ],
      "id": "S8N3mA7z7MHx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "PLdzP6xz7MHx"
      },
      "outputs": [],
      "source": [
        "service_response = llm.invoke(service_messages)"
      ],
      "id": "PLdzP6xz7MHx"
    },
    {
      "cell_type": "code",
      "source": [
        "print(service_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATOHfr8YR1wT",
        "outputId": "f73dd976-cbf3-4e65-9fcb-8ceaaae8df6b"
      },
      "id": "ATOHfr8YR1wT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrr, greetings to ye, valued customer! I be sorry to inform ye that the warranty don't be coverin' the cleanin' expenses for yer kitchen, matey. It seems ye had a bit of a mishap with yer blender, forgettin' to put the lid on before setlin' her to work, savvy? Unfortunately, that be considered misuse, and we can't be held responsible for the mess. Better luck next time, me hearty! May the winds o' fortune blow in yer favor, and may ye have a grand day, nonetheless! Fair winds!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjseJ2fv7ub6"
      },
      "source": [
        "## IV. Output Parsers\n",
        "\n",
        "Let's start with defining how we would like the LLM output to look like:"
      ],
      "id": "hjseJ2fv7ub6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "tags": [],
        "collapsed": true,
        "id": "qMUyWGDT7ub6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1d7830-3c8e-48f7-a0d7-1e5abb98bdf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ],
      "id": "qMUyWGDT7ub6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Setup"
      ],
      "metadata": {
        "id": "xjG5XyD4CNqN"
      },
      "id": "xjG5XyD4CNqN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 540,
        "tags": [],
        "id": "ZD1PszBX7ub6"
      },
      "outputs": [],
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ],
      "id": "ZD1PszBX7ub6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 81,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ff3fcc-e74f-4a38-ae66-ac190ca87122",
        "id": "PyDEgQ3q7ub7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)"
      ],
      "id": "PyDEgQ3q7ub7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 81,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0534c99b-10fe-45d8-882d-4cddc5927a70",
        "id": "x0etXlCd7ub7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "  \"gift\": true,\n",
            "  \"delivery_days\": 2,\n",
            "  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "id": "x0etXlCd7ub7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2345f91f-7362-46b1-b635-a08feab489f7",
        "id": "suAgn6na7ub7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "type(response.content)"
      ],
      "id": "suAgn6na7ub7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "NGpf9_Zp7ub7"
      },
      "outputs": [],
      "source": [
        "# You will get an error by running this line of code\n",
        "# because'gift' is not a dictionary\n",
        "# 'gift' is a string\n",
        "\n",
        "# DON'T WORRY ABOUT THIS ERROR\n",
        "# response.content.get('gift')"
      ],
      "id": "NGpf9_Zp7ub7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs_uykyj7ub8"
      },
      "source": [
        "### Parse the LLM output string into a Python dictionary"
      ],
      "id": "bs_uykyj7ub8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "MOhKIPuq7ub8"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
      ],
      "id": "MOhKIPuq7ub8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 353,
        "tags": [],
        "id": "OumOdSH_7ub8"
      },
      "outputs": [],
      "source": [
        "gift_schema = ResponseSchema(\n",
        "    name=\"gift\",\n",
        "    description=\"Was the item purchased as a gift for someone else? \"\n",
        "                \"Answer true if yes, false if not or unknown. \"\n",
        "                \"Output data type: Boolean.\"\n",
        ")\n",
        "\n",
        "delivery_days_schema = ResponseSchema(\n",
        "    name=\"delivery_days\",\n",
        "    description=\"How many days did it take for the product to arrive? \"\n",
        "                \"If this information is not found, output -1. \"\n",
        "                \"Output data type: Integer.\"\n",
        ")\n",
        "\n",
        "price_value_schema = ResponseSchema(\n",
        "    name=\"price_value\",\n",
        "    description=\"Extract any sentences about the value or price, \"\n",
        "                \"and output them as a comma-separated Python list. \"\n",
        "                \"Output data type: List of strings.\"\n",
        ")\n",
        "\n",
        "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n"
      ],
      "id": "OumOdSH_7ub8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "av5MzFyw7ub8"
      },
      "outputs": [],
      "source": [
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "id": "av5MzFyw7ub8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "xhY2BzTN7ub8"
      },
      "outputs": [],
      "source": [
        "format_instructions = output_parser.get_format_instructions()"
      ],
      "id": "xhY2BzTN7ub8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d72de7-1c16-489e-c861-4584afb0c079",
        "id": "amJWrrTV7ub8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer true if yes, false if not or unknown. Output data type: Boolean.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1. Output data type: Integer.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma-separated Python list. Output data type: List of strings.\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(format_instructions)"
      ],
      "id": "amJWrrTV7ub8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 370,
        "tags": [],
        "id": "oj_ZtdnZ7ub8"
      },
      "outputs": [],
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, \\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(\n",
        "    text=customer_review,\n",
        "    format_instructions=format_instructions\n",
        ")"
      ],
      "id": "oj_ZtdnZ7ub8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332354ce-ce12-4b47-cdff-1c6cd961b43e",
        "id": "BAQozNEj7ub9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "\n",
            "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
            "\n",
            "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
            "\n",
            "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased as a gift for someone else? Answer true if yes, false if not or unknown. Output data type: Boolean.\n",
            "\t\"delivery_days\": string  // How many days did it take for the product to arrive? If this information is not found, output -1. Output data type: Integer.\n",
            "\t\"price_value\": string  // Extract any sentences about the value or price, and output them as a comma-separated Python list. Output data type: List of strings.\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(messages[0].content)"
      ],
      "id": "BAQozNEj7ub9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "T3zzLQNp7ub9"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(messages)"
      ],
      "id": "T3zzLQNp7ub9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c54bd308-2324-4ab6-a845-2124d809e4f1",
        "id": "YVUju3y47ub9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"gift\": true,\n",
            "\t\"delivery_days\": 2,\n",
            "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "print(response.content)"
      ],
      "id": "YVUju3y47ub9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "vgW5uiIC7ub9"
      },
      "outputs": [],
      "source": [
        "output_dict = output_parser.parse(response.content)"
      ],
      "id": "vgW5uiIC7ub9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e90bba-3eaa-4bef-debe-d114709c784c",
        "id": "1jn4dEUp7ub9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': True,\n",
              " 'delivery_days': 2,\n",
              " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "output_dict"
      ],
      "id": "1jn4dEUp7ub9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2165ae94-3b7c-4013-8ca9-7a865b6364dd",
        "id": "Wk4isW4A7ub9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Congratulations! You now have a Python dict instead of str.\n",
        "# This is the power of LangChain's output parser.\n",
        "\n",
        "type(output_dict)"
      ],
      "id": "Wk4isW4A7ub9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "715774dc-4edd-4715-94f9-94bf6f2f3605",
        "id": "Vuv12ptJ7ub-"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "output_dict.get('delivery_days')"
      ],
      "id": "Vuv12ptJ7ub-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra: Put all together with LCEL (LangChain Expression Language)"
      ],
      "metadata": {
        "id": "hNXiMpaoBBXI"
      },
      "id": "hNXiMpaoBBXI"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
      ],
      "metadata": {
        "id": "UKX_Oj-oi4zC"
      },
      "id": "UKX_Oj-oi4zC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = {\n",
        "    \"model\": \"llama-3.3-70b-versatile\",\n",
        "    \"model_provider\": \"groq\",\n",
        "    \"temperature\": 0\n",
        "}\n",
        "\n",
        "llm = init_chat_model(**configs)"
      ],
      "metadata": {
        "id": "fCB8__inDtdu"
      },
      "id": "fCB8__inDtdu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price, \\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=prompt_template)"
      ],
      "metadata": {
        "id": "YcFARsvVDd06"
      },
      "id": "YcFARsvVDd06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gift_schema = ResponseSchema(\n",
        "    name=\"gift\",\n",
        "    description=\"Was the item purchased as a gift for someone else? \"\n",
        "                \"Answer true if yes, false if not or unknown. \"\n",
        "                \"Output data type: Boolean.\"\n",
        ")\n",
        "\n",
        "delivery_days_schema = ResponseSchema(\n",
        "    name=\"delivery_days\",\n",
        "    description=\"How many days did it take for the product to arrive? \"\n",
        "                \"If this information is not found, output -1. \"\n",
        "                \"Output data type: Integer.\"\n",
        ")\n",
        "\n",
        "price_value_schema = ResponseSchema(\n",
        "    name=\"price_value\",\n",
        "    description=\"Extract any sentences about the value or price, \"\n",
        "                \"and output them as a comma-separated Python list. \"\n",
        "                \"Output data type: List of strings.\"\n",
        ")\n",
        "\n",
        "response_schemas = [gift_schema, delivery_days_schema, price_value_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "U7ZJMyQ5Dqom"
      },
      "id": "U7ZJMyQ5Dqom",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partial_prompt = prompt.partial(format_instructions=output_parser.get_format_instructions)"
      ],
      "metadata": {
        "id": "dVtrSjjtjinZ"
      },
      "id": "dVtrSjjtjinZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = partial_prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "z3aOMzvMBYHa"
      },
      "id": "z3aOMzvMBYHa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "COZNaqWkBYqM"
      },
      "id": "COZNaqWkBYqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"text\": customer_review})\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UwWnakyGb8h",
        "outputId": "a3ed47af-826a-4de3-c750-f3653a850bf7"
      },
      "id": "6UwWnakyGb8h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': True,\n",
              " 'delivery_days': 2,\n",
              " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYKhca55DWLm",
        "outputId": "b1a48857-7dcb-425c-bf94-e4409c543fb6"
      },
      "id": "IYKhca55DWLm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3c9212"
      },
      "source": [
        "* * *\n",
        "# Summary of Your LangChain Learning Journey\n",
        "\n",
        "In this notebook, you've covered the fundamental building blocks of creating powerful applications with LangChain. Here's a breakdown of the key concepts:\n",
        "\n",
        "**1. Setting Up Your Environment:**\n",
        "\n",
        "* You started by installing the necessary libraries, including:\n",
        "- `python-dotenv` to manage your API keys securely\n",
        "- `langchain-groq` to initialize the ChatGroq class with init_chat_model\n",
        "* You learned how to load your API keys from a `.env` file, which is a best practice for keeping your credentials safe and separate from your code.\n",
        "\n",
        "**2. Interacting with Language Models (LLMs):**\n",
        "\n",
        "* You instantiated a chat model from `init_chat_model()`.\n",
        "* You sent a simple message to the LLM and received a response, demonstrating the basic request-response cycle.\n",
        "\n",
        "**3. Crafting Precise Prompts with Prompt Templates:**\n",
        "\n",
        "* You discovered the power of `ChatPromptTemplate` to create reusable and dynamic prompts. This allows you to define a template with placeholders (like `{style}` and `{text}`) and then fill them in with specific values.\n",
        "* You saw how to format the prompt template with different styles and text, creating tailored messages to send to the LLM. This is crucial for guiding the model's output and achieving your desired results.\n",
        "\n",
        "**4. Structuring and Parsing LLM Outputs:**\n",
        "\n",
        "* You encountered a common challenge: the LLM's output is often a raw string, which can be difficult to work with programmatically.\n",
        "* You learned how to use `StructuredOutputParser` and `ResponseSchema` to define a specific JSON structure for the LLM's response.\n",
        "* By providing these format instructions in your prompt, you guided the LLM to generate a well-formed JSON object.\n",
        "* Finally, you used the `output_parser` to parse the LLM's string response into a Python dictionary, making it easy to access and use the extracted information.\n",
        "\n",
        "**5. Chaining it all together with LangChain Expression Language (LCEL):**\n",
        "\n",
        "*   You learned how to use the pipe symbol (`|`) to chain together the prompt, the model, and the output parser into a single, elegant pipeline. This makes your code more concise and readable.\n",
        "\n",
        "**In essence, you've learned how to:**\n",
        "\n",
        "* **Connect** to a powerful language model.\n",
        "* **Communicate** your instructions effectively using prompt templates.\n",
        "* **Control** the output format to get structured, usable data.\n",
        "* **Chain** these components together to create a streamlined workflow.\n",
        "\n",
        "These are the foundational skills you'll need to build more complex and sophisticated LangChain applications. Keep up the great work!"
      ],
      "id": "ba3c9212"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dca47fb"
      },
      "source": [
        "* * *\n",
        "# Key Commands and Imports to Remember\n",
        "\n",
        "### Python Libraries:\n",
        "- **`import os`**: Interacts with the operating system, mainly for accessing environment variables.\n",
        "- **`from dotenv import load_dotenv`**: Loads environment variables from a `.env` file to securely manage API credentials.\n",
        "\n",
        "### LangChain Libraries:\n",
        "- **`from langchain.chat_models import init_chat_model`**: Easily initializes a chat model instance.\n",
        "- **`from langchain_core.messages import HumanMessage`**: Represents user messages in the chat interaction.\n",
        "- **`from langchain_core.prompts import ChatPromptTemplate`**: For creating and formatting prompt templates.\n",
        "- **`from langchain.output_parsers import ResponseSchema, StructuredOutputParser`**: For parsing and structuring LLM outputs.\n",
        "\n",
        "### Key LangChain Classes and Functions:\n",
        "- **`init_chat_model()`**: Easily initializes a chat model instance.\n",
        "- **`HumanMessage()`**: Used to create user messages in the chat interaction.\n",
        "- **`ResponseSchema()`**: Defines a schema for expected responses, including name and description.  It's good practice to specify the output data type in the description.\n",
        "- **`ChatPromptTemplate.from_template()`**: Convenient method to create a prompt template from a string.\n",
        "- **`prompt_template.format_messages()`**: Formats the prompt template with specific values (e.g., style, text).\n",
        "- **`prompt.partial()`**: A method to partially format a prompt template, which is useful when you want to pre-fill some of the variables.\n",
        "- **`StructuredOutputParser.from_response_schemas()`**: Creates an output parser based on response schemas.\n",
        "- **`output_parser.get_format_instructions()`**: Generates formatting instructions for the LLM output.\n",
        "- **`output_parser.parse()`**: Parses the LLM output (string) into a Python dictionary."
      ],
      "id": "5dca47fb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}