{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a786c77c"
      },
      "source": [
        "# Lesson 2: Memory\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/bqtankiet/langchain-llm-course/blob/main/L2_Memory.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "This notebook documents my learning journey on **LangChain for LLM Application Development** course from Deeplearning.ai \\\n",
        "[Lesson 2: Memory](https://learn.deeplearning.ai/courses/langchain/lesson/ls57z/memory)\n",
        "\n",
        "\\\n",
        "What I Learned\n",
        "- The Importance of Memory: I learned why memory is crucial for building conversational AI and how LLMs are inherently stateless.\n",
        "- Manual Memory Implementation: I saw how to build a basic memory system from scratch using a simple Python list to store conversation history.\n",
        "- Legacy LangChain Memory (Deprecated): I explored the now-deprecated memory modules in LangChain, such as `ConversationBufferMemory`, `ConversationBufferWindowMemory`, `ConversationTokenBufferMemory`, and `ConversationSummaryBufferMemory`, to understand the evolution of memory management.\n",
        "- The Modern Approach with `RunnableWithMessageHistory`: I learned how to use the recommended `RunnableWithMessageHistory` to create stateful, session-aware conversational chains.\n",
        "- Customizing Memory with `BaseChatMessageHistory`: I discovered how to create a custom memory class by inheriting from `BaseChatMessageHistory`, allowing for tailored memory management and summarization logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Setting up the Environment"
      ],
      "metadata": {
        "id": "tSDO3TsSkH-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain-groq"
      ],
      "metadata": {
        "id": "kRGUJsGskLCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 147,
        "tags": [],
        "id": "a1f518f5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "_ = load_dotenv(override=True) # read local .env file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Define our ChatModel"
      ],
      "metadata": {
        "id": "JyIZwHqwoCrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\n",
        "    model = \"llama-3.3-70b-versatile\",\n",
        "    model_provider = \"groq\",\n",
        "    temperature = 0\n",
        ")"
      ],
      "metadata": {
        "id": "DVY-V5thkqUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a simple assistant. Respond in a simple and very short way.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "7SqpIEWeyXtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. What happens if we don't use Memory?"
      ],
      "metadata": {
        "id": "TmbNhmC7oRz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "def simple_chat(text):\n",
        "  response = chain.invoke([HumanMessage(content=text)])\n",
        "  print(response.content)"
      ],
      "metadata": {
        "id": "skZV-0donVDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_chat(\"Hello, my name is Ken\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWh01LXsofFo",
        "outputId": "590a7d1d-7f01-4617-b74d-01ea18fb14c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Ken.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_chat(\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxBSIjha9o9k",
        "outputId": "840de8f4-e927-4cee-8b08-3691eedc1115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_chat(\"What is my name?\") # The LLM don't remember my name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIVpBXC7ol7t",
        "outputId": "e7cdd963-acf7-4670-f431-ac7f0c0af184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV. Let's try to build a Simple Memory\n"
      ],
      "metadata": {
        "id": "dEAiVwfq8src"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "def memory_chat(text):\n",
        "    global messages\n",
        "    messages.append(HumanMessage(content=text))\n",
        "    response = chain.invoke(messages)\n",
        "    messages.append(response)\n",
        "    print(response.content)"
      ],
      "metadata": {
        "id": "XPerFdFn8s8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory_chat(\"Hello, my name is Ken\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtAPx9ph9Qkr",
        "outputId": "d3ff3a3c-7d7a-4489-b50d-0cc40c6b81c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Ken.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory_chat(\"What is 1+1?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JezNXfPo9tdV",
        "outputId": "7293d86b-ebdb-469d-d8b7-89ca77cf83c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory_chat(\"What is my name?\") # Yeah! The model remembers my name correctly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_XUSY-G9WDZ",
        "outputId": "abb847e8-11d3-483a-982e-24a703be862d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just see the overall conversation memory\n",
        "for m in messages:\n",
        "  role = m.__class__.__name__\n",
        "  print(f'{role}: {m.content}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpzgj9hZ-jnw",
        "outputId": "66978fe9-3e86-48a7-ae10-95a6f79c684e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HumanMessage: Hello, my name is Ken\n",
            "AIMessage: Hi Ken.\n",
            "HumanMessage: What is 1+1?\n",
            "AIMessage: 2\n",
            "HumanMessage: What is my name?\n",
            "AIMessage: Ken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V. Experimenting with deprecated LangChain Memory Types\n",
        "Note: All ConversationMemory types used below are deprecated in LangChain v0.3+\n",
        "It's recommended to use LCEL with `RunnableWithMessageHistory` and manage trimming/summarizing yourself.\n"
      ],
      "metadata": {
        "id": "Dwl07RFQ-UVp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1297dcd5"
      },
      "source": [
        "### ConversationBufferMemory\n",
        "(deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 96,
        "tags": [],
        "id": "20ad6fe2"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 132,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88bdf13d",
        "outputId": "1f255571-cad4-46a2-868d-88bc90d4c762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-1620522523.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/tmp/ipython-input-16-1620522523.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=chain,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 45,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "db24677d",
        "outputId": "b0c34bf6-3a6b-4691-b6a9-223aedbdc81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Ken\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Ken, nice to meet you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Ken\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "cc3ef937",
        "outputId": "6639da2f-c57d-4874-b33a-c85b7d71330a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Ken\n",
            "AI: Hello Ken, nice to meet you.\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "acf3339a",
        "outputId": "e70f0fba-3a2d-4296-bc94-f364ed898ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Ken\n",
            "AI: Hello Ken, nice to meet you.\n",
            "Human: What is 1+1?\n",
            "AI: The answer is 2.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ken.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2529400d",
        "outputId": "446e1459-dca1-4c40-a709-6b5775fa78c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Ken\n",
            "AI: Hello Ken, nice to meet you.\n",
            "Human: What is 1+1?\n",
            "AI: The answer is 2.\n",
            "Human: What is my name?\n",
            "AI: Ken.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5018cb0a",
        "outputId": "78651254-7606-4fd4-cc1c-9ee157797d0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi, my name is Ken\\nAI: Hello Ken, nice to meet you.\\nHuman: What is 1+1?\\nAI: The answer is 2.\\nHuman: What is my name?\\nAI: Ken.'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "14219b70"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "a36e9905"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61631b1f",
        "outputId": "042f4458-5930-4e9b-f6f6-ab3534468f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2fdf9ec",
        "outputId": "581830ab-0dbb-45bc-c705-b9d29e97b933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 62,
        "tags": [],
        "id": "7ca79256"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "890a4497",
        "outputId": "785a0522-5263-4c89-bb4c-fa38dd3c3867"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=chain,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "conversation.predict(input=\"What did you say when I said Hi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "TK1WShVT5Csa",
        "outputId": "78280d4c-ac97-401a-e967-751188ec6f29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi\n",
            "AI: What's up\n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n",
            "Human: What did you say when I said Hi\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I said \"What\\'s up\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cf98e9ff"
      },
      "source": [
        "### ConversationBufferWindowMemory\n",
        "(deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 45,
        "tags": [],
        "id": "66eeccc3"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 45,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ea6233e",
        "outputId": "3e51dce0-3628-4968-b2e7-5a10490611dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-30-802562642.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=1)\n"
          ]
        }
      ],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 113,
        "tags": [],
        "id": "dc4553fb"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a788403",
        "outputId": "9bb856e6-7d1a-4dcd-f578-171aac3397f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "4087bc87"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=chain,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 45,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4faaa952",
        "outputId": "dd2af647-ce8d-4b36-eda8-ebb65a72125d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Ken, nice to meet you.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Ken\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bb20ddaa",
        "outputId": "58473cbb-f487-4d10-a4be-8ce950a69fc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "489b2194",
        "outputId": "2ceeea5f-9c5e-4b8e-89c7-52331758fe72"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2931b92"
      },
      "source": [
        "### ConversationTokenBufferMemory\n",
        "(deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 79,
        "tags": [],
        "id": "fb9020ed"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 147,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "716c5c86a7c44ee8ae319600a2c54f44",
            "ccc5512aa2e54710ab82a3f0ac8f446b",
            "41cd9624614446ca9828468e9cd37ea9",
            "b0c3b709ef744bacb209acb137759954",
            "c27d865e26e54861bdbe38fdee6c6db7",
            "31a52942f7934dae8088fc7ded608890",
            "0dff69d677d845fdb384aa472483a2cb",
            "d996ba558bec47d687f3abb995c27c92",
            "09d105626c3b4e41980667cead0e383c",
            "e624f32ffcc54dfd9fdc9637cd7331b7",
            "914ef4d93d14483f80ac35af6335a973",
            "14abbeae891b4a97b72d10693d983cd5",
            "0c478688e25e4b48990c497914d76898",
            "a226083d836842a2be0a1c1d8f7dca9e",
            "6bd65aee65bc4421b6010111f405d030",
            "52cb3bc3e55349cbb4be0e250e3afa8b",
            "fda385c988854e3ba5c157aa6fbe5904",
            "79e40cef7f6f4d46aae13cc91d998d36",
            "36efb5498eb84ddd86c1c6304f12e256",
            "e807995c982e42a98f20cea3167de1e8",
            "d197da1c795245e3bcb566bbe0aedc5c",
            "e7a0fd67618f4802a002f74e0c0cc043",
            "afb4c1b671624db8b85520949f459048",
            "d4f2a7d70e1b423295952eca9c6942f6",
            "d1860a679ced40ce9675f9fa2de2a101",
            "83890cdd320a463c8b87aa872159e618",
            "735bb5a30338458d96da40d7cb1e0e1c",
            "78a9eef1f3464427a8d3dd89303f0f78",
            "98eadf584740443291df4c894c1843b1",
            "c9eddea1b620450aaaa8f472df854236",
            "9fe5e6cba8f3479ca7e8ae1d7c5ee40a",
            "35f5c57fc95b4e1987dce4df9a446cf6",
            "321a8b58f725415aa56f2972b72827d4",
            "1c91f24db0e447bc910184a5b1dd15cc",
            "b738faf65cdc4108862a48330fffc8d8",
            "f20ea2f6dd7147f6aba0214d1a1036e7",
            "75033235e9c447dd90bc2119fc122afb",
            "2a93ff5c753f4237a074cee9771e8ee1",
            "4c37d84835c24a05ba4dd454331225dc",
            "24a1a55d4e24457aa8b4fa11e74f4785",
            "d7edf4c6962f430aa5f357e694d5e187",
            "597152fc15584427a4e8e53bc48a651c",
            "b9fc81e0865642d8982beb666f597766",
            "e977989c025e469e880709ca2f1aa027",
            "9fab1c52fa3a45d0acdc9d63eb071cdf",
            "dd4e69a0a4154e74a5f4364f50f00673",
            "862a68fb1b394e4b975743198f4344bd",
            "c567acd2ad724095ae56455c4b38790c",
            "4df9b6b5002a4dfeada054d4e94f2549",
            "f45604baa9924c82b3bad74cbb0fb776",
            "893a18b24f2c486b83a4e291f4be87e4",
            "72188acb0e6344e79802046b116ebb76",
            "fe836cb9911546489460d3b83277368f",
            "2b464654600c4ea98843bb3ea513572e",
            "e79692d5a762458da797b4fad64cfb5e"
          ]
        },
        "id": "43582ee6",
        "outputId": "1f67246d-959a-4e97-8047-755cf175dfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-38-2862484728.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=25)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "716c5c86a7c44ee8ae319600a2c54f44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14abbeae891b4a97b72d10693d983cd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afb4c1b671624db8b85520949f459048"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c91f24db0e447bc910184a5b1dd15cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fab1c52fa3a45d0acdc9d63eb071cdf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=25)\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "284288e1",
        "outputId": "617dc301-b62d-4967-ebb4-25428c830eb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ff55d5d"
      },
      "source": [
        "### ConversationSummaryMemory\n",
        "(deprecated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 45,
        "tags": [],
        "id": "72dcf8b1"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 283,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a5b238f",
        "outputId": "5c646d1a-3069-4145-b7d1-e8f66d33cfe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-41-1387621936.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
          ]
        }
      ],
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e4ecabe",
        "outputId": "1f967d18-9b05-43d4-800b-8b1ece7b1b55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human greets the AI, and they exchange casual small talk, with the human eventually asking about the schedule for the day.\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "6728edba"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 45,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "9a221b1d",
        "outputId": "4911473d-7c88-4bb3-a3b2-bbd4cfdb16ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human greets the AI, and they exchange casual small talk, with the human eventually asking about the schedule for the day.\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'For the demo, I would recommend showing the latest advancements in natural language processing, specifically the capabilities of our LLM (Large Language Model) in generating human-like text and answering complex questions. You could also showcase its ability to understand and respond to voice commands, which is a key feature that has been improved in the latest update.\\n\\nOne idea for a demo could be to ask the customer to provide a topic or question, and then use the LLM to generate a short article or response on the spot. This would demonstrate the model\\'s ability to think creatively and provide relevant information in real-time.\\n\\nAdditionally, you could also show some of the pre-built applications that we have developed using the LLM, such as the text summarization tool or the chatbot interface. These demos would give the customer a better understanding of how our technology can be applied in real-world scenarios and provide value to their business.\\n\\nWe also have some pre-prepared demos that I can provide you with, such as the \"Conversational AI\" demo, which showcases the LLM\\'s ability to engage in natural-sounding conversations, or the \"Language Translation\" demo, which demonstrates the model\\'s ability to translate text in real-time. Which one of these demos sounds most interesting to you, or do you have a different idea for a demo in mind?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb582617",
        "outputId": "d213820c-8bdc-4ca9-ecfc-5a87879974a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'System: The human greets the AI, and they exchange casual small talk, with the human eventually asking about the schedule for the day. The AI informs the human of a meeting at 8am with the product team, where the human will need to have their PowerPoint presentation prepared, followed by time to work on the LangChain project from 9am-12pm. At noon, the human has lunch with a customer at an Italian restaurant, where they will discuss the latest in AI and demonstrate the latest LLM demo, for which the AI recommends showing the advancements in natural language processing, such as generating human-like text, answering complex questions, and understanding voice commands. The AI suggests demo ideas, including generating a short article or response on the spot, showcasing pre-built applications like text summarization or chatbot interfaces, or using pre-prepared demos like \"Conversational AI\" or \"Language Translation\", and asks the human to choose a demo or share their own idea.'}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VI. RunnableWithMessageHistory\n",
        "`RunnableWithMessageHistory` is the modern approach in LangChain 0.3.x"
      ],
      "metadata": {
        "id": "5LOICapraPjT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "4ba827aa"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "\n",
        "role = \"You are a simple assistant. Respond in a simple and very short way\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", role),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | llm | parser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id):\n",
        "  if session_id not in chat_map:\n",
        "    chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "  return chat_map[session_id]"
      ],
      "metadata": {
        "id": "tLSF4TA5b5GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "history_chat = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history=get_chat_history,\n",
        "    history_messages_key=\"history\",\n",
        "    input_message_key=\"input\"\n",
        ")"
      ],
      "metadata": {
        "id": "Nb_JvmgicUGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_chat.invoke(\n",
        "    {\"input\": \"Hi, my name is Ken\"},\n",
        "    config={\"session_id\": \"111\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BTH4o20nc6n1",
        "outputId": "849ed9c8-7dcd-4ac8-b115-251f9e19a621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Ken'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_chat.invoke(\n",
        "    {\"input\": \"What is 1+1?\"},\n",
        "    config={\"session_id\": \"111\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JdP4Kdc6gN6L",
        "outputId": "4163763c-a7d9-463d-8ce7-498974b1d587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_chat.invoke(\n",
        "    {\"input\": \"What is my name?\"},\n",
        "    config={\"session_id\": \"111\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6fyxOC9WgPyh",
        "outputId": "ed815787-e700-4d07-b0a9-11ab21893e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ken'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_chat.invoke(\n",
        "    {\"input\": \"What is my name?\"},\n",
        "    config={\"session_id\": \"222\"} # another session_id\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0CP9Rbp7gfIt",
        "outputId": "9745504e-6716-4137-a728-5240bf598d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = chat_map[\"111\"]\n",
        "print(type(history))\n",
        "history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6ZKTkOsj38t",
        "outputId": "60c57f7e-0424-4c1d-b5d8-87e0a127ee37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.chat_history.InMemoryChatMessageHistory'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is Ken', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi Ken', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is 1+1?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='2', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Ken', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra: Customizing BaseChatMessageHistory\n"
      ],
      "metadata": {
        "id": "zjADkiBCroJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class ConversationSummaryHistory(BaseChatMessageHistory):\n",
        "  def __init__(self, llm):\n",
        "    self.messages = []\n",
        "    self.llm = llm\n",
        "\n",
        "  def add_message(self, messages):\n",
        "\n",
        "    # Print the details\n",
        "    print(self.messages)\n",
        "    print(f\"{messages.type}: {messages.content}\")\n",
        "\n",
        "    summary_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "          \"You are an expert conversation summarizer. Your task is to generate an updated summary \"\n",
        "          \"of the conversation, given:\\n\"\n",
        "          \"- The existing summary\\n\"\n",
        "          \"- A list of new messages with speaker roles (either 'Human' or 'AI')\\n\\n\"\n",
        "          \"Update the summary to include the new information, keeping it concise but detailed. \"\n",
        "          \"Preserve important facts, names, decisions, and questions. \"\n",
        "          \"Do not lose prior context. Just return the updated summary without any additional explanation.\"),\n",
        "        (\"human\",\n",
        "         \"Existing conversation summary: \\n{summary}\\n\"\n",
        "         \"{role} messages: \\n{messages}\")\n",
        "    ])\n",
        "\n",
        "    summary_messages = summary_prompt.format_messages(\n",
        "        summary=self.messages,\n",
        "        role=messages.type,\n",
        "        messages=messages.content)\n",
        "    summary_response = llm.invoke(summary_messages)\n",
        "    self.messages = [SystemMessage(content = summary_response.content)]\n",
        "\n",
        "  def clear(self):\n",
        "    self.messages=[]"
      ],
      "metadata": {
        "id": "fj1cwDG3sNN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = {}\n",
        "def get_chat_history(session_id):\n",
        "  if session_id not in chat_history:\n",
        "    chat_history[session_id] = ConversationSummaryHistory(llm)\n",
        "  return chat_history[session_id]"
      ],
      "metadata": {
        "id": "LLgLToCcwznI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_history_chat = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history=get_chat_history,\n",
        "    history_messages_key=\"history\",\n",
        "    input_messages_key=\"input\"\n",
        ")"
      ],
      "metadata": {
        "id": "U8bFKvYkxGNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "get_chat_history(\"111\").add_message(HumanMessage(content=\"Hello\"))\n",
        "get_chat_history(\"111\").add_message(AIMessage(content=\"What's up\"))\n",
        "get_chat_history(\"111\").add_message(HumanMessage(content=\"Not much, just hanging\"))\n",
        "get_chat_history(\"111\").add_message(AIMessage(content=\"Cool\"))\n",
        "get_chat_history(\"111\").add_message(HumanMessage(content=\"What is on the schedule today?\"))\n",
        "get_chat_history(\"111\").add_message(AIMessage(content=schedule))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_kMo_EjxmmO",
        "outputId": "2dd0fe81-7c53-44a4-a9d8-ae18be5f1810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "human: Hello\n",
            "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\".', additional_kwargs={}, response_metadata={})]\n",
            "ai: What's up\n",
            "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\".', additional_kwargs={}, response_metadata={})]\n",
            "human: Not much, just hanging\n",
            "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\". The human then replied, \"Not much, just hanging\".', additional_kwargs={}, response_metadata={})]\n",
            "ai: Cool\n",
            "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\". The human then replied, \"Not much, just hanging\". The AI then said \"Cool\".', additional_kwargs={}, response_metadata={})]\n",
            "human: What is on the schedule today?\n",
            "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\". The human then replied, \"Not much, just hanging\". The AI then said \"Cool\". The human asked about their schedule, inquiring \"What is on the schedule today?\"', additional_kwargs={}, response_metadata={})]\n",
            "ai: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_chat_history(\"111\").messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InRjtjvrzOYf",
        "outputId": "2f906ca6-bd9c-4e75-d83d-5a1f42b36580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\". The human then replied, \"Not much, just hanging\". The AI then said \"Cool\". The human asked about their schedule, inquiring \"What is on the schedule today?\" The AI replied that there is a meeting at 8am with the product team, requiring a prepared PowerPoint presentation. From 9am-12pm, the human has time to work on their LangChain project. At Noon, the human has lunch at the Italian restaurant with a customer, who is traveling from over an hour away to discuss the latest in AI, and the human should bring their laptop to show the latest LLM demo.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 254
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary_history_chat.invoke(\n",
        "    {\"input\": \"What would be a good demo to show? And why?\"},\n",
        "    config={\"session_id\":\"111\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "S4XG5KZezrBQ",
        "outputId": "92898b0d-c1b8-4a8d-da35-5a412f5a518c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\". The human then replied, \"Not much, just hanging\". The AI then said \"Cool\". The human asked about their schedule, inquiring \"What is on the schedule today?\" The AI replied that there is a meeting at 8am with the product team, requiring a prepared PowerPoint presentation. From 9am-12pm, the human has time to work on their LangChain project. At Noon, the human has lunch at the Italian restaurant with a customer, who is traveling from over an hour away to discuss the latest in AI, and the human should bring their laptop to show the latest LLM demo.', additional_kwargs={}, response_metadata={})]\n",
            "human: What would be a good demo to show? And why?\n",
            "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\". The human then replied, \"Not much, just hanging\". The AI then said \"Cool\". The human asked about their schedule, inquiring \"What is on the schedule today?\" The AI replied that there is a meeting at 8am with the product team, requiring a prepared PowerPoint presentation. From 9am-12pm, the human has time to work on their LangChain project. At Noon, the human has lunch at the Italian restaurant with a customer, who is traveling from over an hour away to discuss the latest in AI, and the human should bring their laptop to show the latest LLM demo. The human is now seeking advice on what would be a good demo to show the customer and why, in the context of their lunch meeting to discuss the latest in AI.', additional_kwargs={}, response_metadata={})]\n",
            "ai: LLM text generation demo. It's impressive and relevant.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"LLM text generation demo. It's impressive and relevant.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_chat_history(\"111\").messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUjCJwTX0JFF",
        "outputId": "75b49280-b34c-42bb-8621-196e080ff49a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='The conversation started with a greeting from a human who said \"Hello\". The AI responded with \"What\\'s up\". The human then replied, \"Not much, just hanging\". The AI then said \"Cool\". The human asked about their schedule, inquiring \"What is on the schedule today?\" The AI replied that there is a meeting at 8am with the product team, requiring a prepared PowerPoint presentation. From 9am-12pm, the human has time to work on their LangChain project. At Noon, the human has lunch at the Italian restaurant with a customer, who is traveling from over an hour away to discuss the latest in AI, and the human should bring their laptop to show the latest LLM demo. The human sought advice on what would be a good demo to show the customer and why, in the context of their lunch meeting to discuss the latest in AI. The AI suggested an LLM text generation demo, noting it\\'s impressive and relevant.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303872d5"
      },
      "source": [
        "---\n",
        "# Summary of Your LangChain Learning Journey: Memory\n",
        "\n",
        "In this notebook, you've embarked on a comprehensive exploration of **Memory** in LangChain, a critical component for building intelligent, stateful conversational AI. Here's a recap of the key concepts you've mastered:\n",
        "\n",
        "**1. The \"Why\" of Memory:**\n",
        "\n",
        "*   You started by confronting the inherent **statelessness** of LLMs, observing that without a memory mechanism, they cannot recall past interactions. This highlighted the fundamental need for memory in conversational applications.\n",
        "\n",
        "**2. Building Memory from Scratch:**\n",
        "\n",
        "*   You gained a foundational understanding of memory by implementing a **Simple Memory** using a basic Python list. This hands-on exercise demystified the core principles of storing and retrieving conversation history.\n",
        "\n",
        "**3. A Look at Legacy Memory Modules (Deprecated):**\n",
        "\n",
        "*   You journeyed through LangChain's history, experimenting with several of its original, now-deprecated memory modules. This provided valuable context on the evolution of memory management within the library. These legacy modules include:\n",
        "    *   `ConversationBufferMemory`\n",
        "    *   `ConversationBufferWindowMemory`\n",
        "    *   `ConversationTokenBufferMemory`\n",
        "    *   `ConversationSummaryBufferMemory`\n",
        "\n",
        "**4. The Modern Approach: `RunnableWithMessageHistory`**\n",
        "\n",
        "*   You then graduated to the current, recommended approach for managing chat history: `RunnableWithMessageHistory`. You learned how this powerful tool simplifies the process of making your chains stateful and managing conversation histories for multiple sessions.\n",
        "\n",
        "**5. Advanced Customization with `BaseChatMessageHistory`**\n",
        "\n",
        "*   Finally, you delved into advanced customization by creating your own **custom memory class**. By inheriting from `BaseChatMessageHistory`, you learned how to implement bespoke logic for storing and summarizing conversation history, giving you complete control over the memory management process.\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "Your journey through this notebook has equipped you with the skills to:\n",
        "\n",
        "*   **Grasp** the essential role of memory in creating coherent and context-aware conversational agents.\n",
        "*   **Appreciate** the evolution of memory management in LangChain, from its legacy components to its modern, more powerful tools.\n",
        "*   **Implement** both basic and advanced memory solutions, from simple lists to custom, summarization-based memory classes.\n",
        "*   **Confidently apply** the `RunnableWithMessageHistory` class to build robust, scalable, and session-aware conversational applications.\n",
        "\n",
        "You are now well-prepared to build more sophisticated, engaging, and human-like conversational experiences with LangChain. Excellent work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26dc5490"
      },
      "source": [
        "* * *\n",
        "# Key Commands and Imports to Remember\n",
        "\n",
        "### Python Libraries:\n",
        "- **`import os`**: Interacts with the operating system, mainly for accessing environment variables.\n",
        "- **`from dotenv import load_dotenv`**: Loads environment variables from a `.env` file to securely manage API credentials.\n",
        "\n",
        "### LangChain Core Libraries (Modern Approach):\n",
        "- **`from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder`**: Used to create flexible and stateful prompt templates. `MessagesPlaceholder` is key for inserting chat history.\n",
        "- **`from langchain_core.output_parsers.string import StrOutputParser`**: A simple parser to get the string content from the LLM's output.\n",
        "- **`from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory`**: `InMemoryChatMessageHistory` is a simple, in-memory way to store chat history. `BaseChatMessageHistory` is the base class for creating custom chat history classes.\n",
        "- **`from langchain_core.messages import BaseMessage`**: The base class for all message types, useful when creating custom history objects.\n",
        "- **`from langchain_core.runnables.history import RunnableWithMessageHistory`**: The main class for wrapping a chain to make it stateful.\n",
        "\n",
        "### Legacy LangChain Libraries (Deprecated):\n",
        "- **`from langchain.chains import ConversationChain`**: The older chain for facilitating conversations with memory.\n",
        "- **`from langchain.memory import ...`**: This module contains various **deprecated** memory types:\n",
        "  - `ConversationBufferMemory`\n",
        "  - `ConversationBufferWindowMemory`\n",
        "  - `ConversationTokenBufferMemory`\n",
        "  - `ConversationSummaryBufferMemory`\n",
        "\n",
        "### Key LangChain Classes and Functions:\n",
        "- **`init_chat_model(...)`**: Initializes your chosen chat model.\n",
        "- **`ChatPromptTemplate.from_messages([...])`**: Creates a prompt template.\n",
        "- **`RunnableWithMessageHistory(...)`**: Wraps a runnable/chain to give it stateful memory. You must provide the runnable, a `get_session_history` function, and specify the input/history keys.\n",
        "- **`history_chat.invoke({\"input\": ...}, config={\"session_id\": ...})`**: Runs the stateful chain for a specific session.\n",
        "- **`class CustomHistory(BaseChatMessageHistory): ...`**: Inherit from `BaseChatMessageHistory` to create your own custom memory management logic."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}